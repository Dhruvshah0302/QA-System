{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aff72608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('C:/Users/Dhruv/Desktop/QA_System/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a9f8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c91fb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found\n"
     ]
    }
   ],
   "source": [
    "if google_api_key == \"\":\n",
    "    print(\"API not found\")\n",
    "else:\n",
    "    print(\"API key found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a28fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-gemini in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (0.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: google-generativeai>=0.5.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-llms-gemini) (0.8.5)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-llms-gemini) (0.13.2)\n",
      "Requirement already satisfied: pillow<11,>=10.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-llms-gemini) (10.4.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.11.10)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2025.3.2)\n",
      "Requirement already satisfied: httpx in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.1.3)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (4.3.7)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.0.39)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.18.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.7)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.179.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-llms-gemini) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-llms-gemini) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.4.8)\n",
      "Requirement already satisfied: click in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (24.2)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-llms-gemini) (3.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-gemini) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-llms-gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0c8f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-gemini in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: google-generativeai>=0.5.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-embeddings-gemini) (0.8.5)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-embeddings-gemini) (0.13.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.11.10)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2025.3.2)\n",
      "Requirement already satisfied: httpx in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.3.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.1.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (4.3.7)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.0.39)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.18.0)\n",
      "Requirement already satisfied: griffe in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.7)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.179.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-generativeai>=0.5.2->llama-index-embeddings-gemini) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.70.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.74.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (1.71.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.4.8)\n",
      "Requirement already satisfied: click in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (24.2)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.5.2->llama-index-embeddings-gemini) (3.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dhruv\\anaconda3\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-embeddings-gemini) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-embeddings-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d2310eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "import google.generativeai as genai\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d666ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9759b095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-03-25',\n",
      "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
      "      description='Gemini 2.5 Pro Preview 03-25',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-05-20',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash',\n",
      "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
      "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-17',\n",
      "      display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
      "      description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-06',\n",
      "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
      "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-06-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-05',\n",
      "      display_name='Gemini 2.5 Pro Preview',\n",
      "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro',\n",
      "      base_model_id='',\n",
      "      version='2.5',\n",
      "      display_name='Gemini 2.5 Pro',\n",
      "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash',\n",
      "      description='Gemini 2.0 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite 001',\n",
      "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite',\n",
      "      description='Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Preview Image Generation',\n",
      "      description='Gemini 2.0 Flash Preview Image Generation',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-pro-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview TTS',\n",
      "      description='Gemini 2.5 Flash Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Pro Preview TTS',\n",
      "      description='Gemini 2.5 Pro Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/learnlm-2.0-flash-experimental',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='LearnLM 2.0 Flash Experimental',\n",
      "      description='LearnLM 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=32768,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-1b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 1B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 4B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-12b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 12B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-27b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 27B',\n",
      "      description='',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E4B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e2b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E2B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash-Lite',\n",
      "      description='Stable verion of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp-03-07',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental 03-07',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n",
      "Model(name='models/imagen-3.0-generate-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Imagen 3.0',\n",
      "      description='Vertex served Imagen 3.0 002 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 (Preview)',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 Ultra (Preview)',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Imagen 4',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-ultra-generate-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Imagen 4 Ultra',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-fast-generate-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Imagen 4 Fast',\n",
      "      description='Vertex served Imagen 4.0 Fast model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-2.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Veo 2',\n",
      "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
      "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
      "                   'https://console.cloud.google.com/billing to enable it.'),\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3',\n",
      "      description='Veo 3 preview.',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-fast-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3 fast',\n",
      "      description='Veo 3 fast preview.',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-live-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description='Gemini 2.0 Flash 001',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-live-2.5-flash-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Live 2.5 Flash Preview',\n",
      "      description='Gemini Live 2.5 Flash Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-live-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash Live Preview',\n",
      "      description='Gemini 2.5 Flash Live Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "for i in genai.list_models():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e8031c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-2.5-flash-lite\n"
     ]
    }
   ],
   "source": [
    "for i in genai.list_models():\n",
    "    if 'generateContent' in i.supported_generation_methods:\n",
    "        print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e56c3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../Data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa979eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is machine learning?\n",
      "Machine learning is a branch of artificial intelligence (AI) and computer science which\n",
      "focuses on the use of data and algorithms to imitate the way that humans learn,\n",
      "gradually improving its accuracy.\n",
      "IBM has a rich history with machine learning. One of its own, Arthur Samuel, is credited\n",
      "for coining the term, machine learning with his research (link resides outside ibm.com)\n",
      "around the game of checkers. Robert Nealey, the self-proclaimed checkers master,\n",
      "played the game on an IBM 7094 computer in 1962, and he lost to the computer.\n",
      "Compared to what can be done today, this feat seems trivial, but its considered a major\n",
      "milestone in the field of artificial intelligence.\n",
      "Over the last couple of decades, the technological advances in storage and processing\n",
      "power have enabled some innovative products based on machine learning, such as\n",
      "Netflixs recommendation engine and self-driving cars.\n",
      "Machine learning is an important component of the growing field of data science.\n",
      "Through the use of statistical methods, algorithms are trained to make classifications or\n",
      "predictions, and to uncover key insights in data mining projects. These insights\n",
      "subsequently drive decision making within applications and businesses, ideally\n",
      "impacting key growth metrics. As big data continues to expand and grow, the market\n",
      "demand for new data scientists will increase. They will be required to help identify the\n",
      "most relevant business questions and the data to answer them.\n",
      "Machine learning algorithms are typically created using frameworks such as Python that\n",
      "accelerate solution development by using platforms like TensorFlow or PyTorch.\n",
      "Now available: watsonx.ai\n",
      "The all-new enterprise studio that brings together traditional machine learning along\n",
      "with new generative AI capabilities powered by foundation models.\n",
      "Try watsonx.ai\n",
      "Begin your journey to AI\n",
      "Learn how to scale AI\n",
      "Explore the AI Academy\n",
      "Machine Learning vs. Deep Learning vs. Neural Networks\n",
      "Since deep learning and machine learning tend to be used interchangeably, its worth\n",
      "noting the nuances between the two. Machine learning, deep learning, and neural\n",
      "networks are all sub-fields of artificial intelligence. However, neural networks is actually\n",
      "a sub-field of machine learning, and deep learning is a sub-field of neural networks.\n",
      "The way in which deep learning and machine learning differ is in how each algorithm\n",
      "learns. \"Deep\" machine learning can use labeled datasets, also known as supervised\n",
      "learning, to inform its algorithm, but it doesnt necessarily require a labeled dataset. The\n",
      "deep learning process can ingest unstructured data in its raw form (e.g., text or images),\n",
      "and it can automatically determine the set of features which distinguish different\n",
      "categories of data from one another. This eliminates some of the human intervention\n",
      "required and enables the use of large amounts of data. You can think of deep learning\n",
      "as \"scalable machine learning\" as Lex Fridman notes in this MIT lecture (link resides\n",
      "outside ibm.com).\n",
      "Classical, or \"non-deep,\" machine learning is more dependent on human intervention to\n",
      "learn. Human experts determine the set of features to understand the differences\n",
      "between data inputs, usually requiring more structured data to learn.\n",
      "Neural networks, or artificial neural networks (ANNs), are comprised of node layers,\n",
      "containing an input layer, one or more hidden layers, and an output layer. Each node, or\n",
      "artificial neuron, connects to another and has an associated weight and threshold. If the\n",
      "output of any individual node is above the specified threshold value, that node is\n",
      "activated, sending data to the next layer of the network. Otherwise, no data is passed\n",
      "along to the next layer of the network by that node. The deep in deep learning is just\n",
      "referring to the number of layers in a neural network. A neural network that consists of\n",
      "more than three layerswhich would be inclusive of the input and the outputcan be\n",
      "considered a deep learning algorithm or a deep neural network. A neural network that\n",
      "only has three layers is just a basic neural network.\n",
      "Deep learning and neural networks are credited with accelerating progress in areas\n",
      "such as computer vision, natural language processing, and speech recognition.\n",
      "See the blog post AI vs. Machine Learning vs. Deep Learning vs. Neural Networks:\n",
      "Whats the Difference? for a closer look at how the different concepts relate.\n",
      "Related content\n",
      "Explore the watsonx.ai interactive demo\n",
      "Download Machine learning for Dummies\n",
      "- This link downloads a pdf\n",
      "Explore Gen AI for developers\n",
      "How does machine learning work?\n",
      "UC Berkeley (link resides outside ibm.com) breaks out the learning system of a\n",
      "machine learning algorithm into three main parts.\n",
      "A Decision Process: In general, machine learning algorithms are used to make a\n",
      "prediction or classification. Based on some input data, which can be labeled or\n",
      "unlabeled, your algorithm will produce an estimate about a pattern in the data.\n",
      "An Error Function: An error function evaluates the prediction of the model. If\n",
      "there are known examples, an error function can make a comparison to assess\n",
      "the accuracy of the model.\n",
      "A Model Optimization Process: If the model can fit better to the data points in the\n",
      "training set, then weights are adjusted to reduce the discrepancy between the\n",
      "known example and the model estimate. The algorithm will repeat this iterative\n",
      "evaluate and optimize process, updating weights autonomously until a\n",
      "threshold of accuracy has been met.\n",
      "Machine learning methods\n",
      "Machine learning models fall into three primary categories.\n",
      "Supervised machine learning\n",
      "Supervised learning, also known as supervised machine learning, is defined by its use\n",
      "of labeled datasets to train algorithms to classify data or predict outcomes accurately.\n",
      "As input data is fed into the model, the model adjusts its weights until it has been fitted\n",
      "appropriately. This occurs as part of the cross validation process to ensure that the\n",
      "model avoids overfitting or underfitting. Supervised learning helps organizations solve a\n",
      "variety of real-world problems at scale, such as classifying spam in a separate folder\n",
      "from your inbox. Some methods used in supervised learning include neural networks,\n",
      "nave bayes, linear regression, logistic regression, random forest, and support vector\n",
      "machine (SVM).\n",
      "Unsupervised machine learning\n",
      "Unsupervised learning, also known as unsupervised machine learning, uses machine\n",
      "learning algorithms to analyze and cluster unlabeled datasets (subsets called clusters).\n",
      "These algorithms discover hidden patterns or data groupings without the need for\n",
      "human intervention. This methods ability to discover similarities and differences in\n",
      "information make it ideal for exploratory data analysis, cross-selling strategies,\n",
      "customer segmentation, and image and pattern recognition. Its also used to reduce the\n",
      "number of features in a model through the process of dimensionality reduction. Principal\n",
      "component analysis (PCA) and singular value decomposition (SVD) are two common\n",
      "approaches for this. Other algorithms used in unsupervised learning include neural\n",
      "networks, k-means clustering, and probabilistic clustering methods.\n",
      "Semi-supervised learning\n",
      "Semi-supervised learning offers a happy medium between supervised and\n",
      "unsupervised learning. During training, it uses a smaller labeled data set to guide\n",
      "classification and feature extraction from a larger, unlabeled data set. Semi-supervised\n",
      "learning can solve the problem of not having enough labeled data for a supervised\n",
      "learning algorithm. It also helps if its too costly to label enough data.\n",
      "For a deep dive into the differences between these approaches, check out \"Supervised\n",
      "vs. Unsupervised Learning: What's the Difference?\"\n",
      "Reinforcement machine learning\n",
      "Reinforcement machine learning is a machine learning model that is similar to\n",
      "supervised learning, but the algorithm isnt trained using sample data. This model learns\n",
      "as it goes by using trial and error. A sequence of successful outcomes will be reinforced\n",
      "to develop the best recommendation or policy for a given problem.\n",
      "The IBM Watson system that won the Jeopardy! challenge in 2011 is a good example.\n",
      "The system used reinforcement learning to learn when to attempt an answer (or\n",
      "question, as it were), which square to select on the board, and how much to\n",
      "wagerespecially on daily doubles.\n",
      "Learn more about reinforcement learning\n",
      "Common machine learning algorithms\n",
      "A number of machine learning algorithms are commonly used. These include:\n",
      "Neural networks: Neural networks simulate the way the human brain works, with\n",
      "a huge number of linked processing nodes. Neural networks are good at\n",
      "recognizing patterns and play an important role in applications including natural\n",
      "language translation, image recognition, speech recognition, and image creation.\n",
      "Linear regression: This algorithm is used to predict numerical values, based on a\n",
      "linear relationship between different values. For example, the technique could be\n",
      "used to predict house prices based on historical data for the area.\n",
      "Logistic regression: This supervised learning algorithm makes predictions for\n",
      "categorical response variables, such as yes/no answers to questions. It can be\n",
      "used for applications such as classifying spam and quality control on a\n",
      "production line.\n",
      "Clustering: Using unsupervised learning, clustering algorithms can identify\n",
      "patterns in data so that it can be grouped. Computers can help data scientists by\n",
      "identifying differences between data items that humans have overlooked.\n",
      "Decision trees: Decision trees can be used for both predicting numerical values\n",
      "(regression) and classifying data into categories. Decision trees use a branching\n",
      "sequence of linked decisions that can be represented with a tree diagram. One of\n",
      "the advantages of decision trees is that they are easy to validate and audit,\n",
      "unlike the black box of the neural network.\n",
      "Random forests: In a random forest, the machine learning algorithm predicts a\n",
      "value or category by combining the results from a number of decision trees.\n",
      "Advantages and disadvantages of machine learning algorithms\n",
      "Depending on your budget, need for speed and precision required, each algorithm\n",
      "typesupervised, unsupervised, semi-supervised, or reinforcementhas its own\n",
      "advantages and disadvantages. For example, decision tree algorithms are used for both\n",
      "predicting numerical values (regression problems) and classifying data into categories.\n",
      "Decision trees use a branching sequence of linked decisions that may be represented\n",
      "with a tree diagram. A prime advantage of decision trees is that they are easier to\n",
      "validate and audit than a neural network. The bad news is that they can be more\n",
      "unstable than other decision predictors.\n",
      "Overall, there are many advantages to machine learning that businesses can leverage\n",
      "for new efficiencies. These include machine learning identifying patterns and trends in\n",
      "massive volumes of data that humans might not spot at all. And this analysis requires\n",
      "little human intervention: just feed in the dataset of interest and let the machine learning\n",
      "system assemble and refine its own algorithmswhich will continually improve with\n",
      "more data input over time. Customers and users can enjoy a more personalized\n",
      "experience as the model learns more with every experience with that person.\n",
      "On the downside, machine learning requires large training datasets that are accurate\n",
      "and unbiased. GIGO is the operative factor: garbage in / garbage out. Gathering\n",
      "sufficient data and having a system robust enough to run it might also be a drain on\n",
      "resources. Machine learning can also be prone to error, depending on the input. With\n",
      "too small a sample, the system could produce a perfectly logical algorithm that is\n",
      "completely wrong or misleading. To avoid wasting budget or displeasing customers,\n",
      "organizations should act on the answers only when there is high confidence in the\n",
      "output.\n",
      "Real-world machine learning use cases\n",
      "Here are just a few examples of machine learning you might encounter every day:\n",
      "Speech recognition: It is also known as automatic speech recognition (ASR), computer\n",
      "speech recognition, or speech-to-text, and it is a capability which uses natural language\n",
      "processing (NLP) to translate human speech into a written format. Many mobile devices\n",
      "incorporate speech recognition into their systems to conduct voice searche.g. Sirior\n",
      "improve accessibility for texting.\n",
      "Customer service: Online chatbots are replacing human agents along the customer\n",
      "journey, changing the way we think about customer engagement across websites and\n",
      "social media platforms. Chatbots answer frequently asked questions (FAQs) about\n",
      "topics such as shipping, or provide personalized advice, cross-selling products or\n",
      "suggesting sizes for users. Examples include virtual agents on e-commerce sites;\n",
      "messaging bots, using Slack and Facebook Messenger; and tasks usually done by\n",
      "virtual assistants and voice assistants.\n",
      "Computer vision: This AI technology enables computers to derive meaningful\n",
      "information from digital images, videos, and other visual inputs, and then take the\n",
      "appropriate action. Powered by convolutional neural networks, computer vision has\n",
      "applications in photo tagging on social media, radiology imaging in healthcare, and\n",
      "self-driving cars in the automotive industry.\n",
      "Recommendation engines: Using past consumption behavior data, AI algorithms can\n",
      "help to discover data trends that can be used to develop more effective cross-selling\n",
      "strategies. Recommendation engines are used by online retailers to make relevant\n",
      "product recommendations to customers during the checkout process.\n",
      "Robotic process automation (RPA): Also known as software robotics, RPA uses\n",
      "intelligent automation technologies to perform repetitive manual tasks.\n",
      "Automated stock trading: Designed to optimize stock portfolios, AI-driven\n",
      "high-frequency trading platforms make thousands or even millions of trades per day\n",
      "without human intervention.\n",
      "Fraud detection: Banks and other financial institutions can use machine learning to spot\n",
      "suspicious transactions. Supervised learning can train a model using information about\n",
      "known fraudulent transactions. Anomaly detection can identify transactions that look\n",
      "atypical and deserve further investigation.\n",
      "Challenges of machine learning\n",
      "As machine learning technology has developed, it has certainly made our lives easier.\n",
      "However, implementing machine learning in businesses has also raised a number of\n",
      "ethical concerns about AI technologies. Some of these include:\n",
      "Technological singularity\n",
      "While this topic garners a lot of public attention, many researchers are not concerned\n",
      "with the idea of AI surpassing human intelligence in the near future. Technological\n",
      "singularity is also referred to as strong AI or superintelligence. Philosopher Nick\n",
      "Bostrum defines superintelligence as any intellect that vastly outperforms the best\n",
      "human brains in practically every field, including scientific creativity, general wisdom,\n",
      "and social skills. Despite the fact that superintelligence is not imminent in society, the\n",
      "idea of it raises some interesting questions as we consider the use of autonomous\n",
      "systems, like self-driving cars. Its unrealistic to think that a driverless car would never\n",
      "have an accident, but who is responsible and liable under those circumstances? Should\n",
      "we still develop autonomous vehicles, or do we limit this technology to\n",
      "semi-autonomous vehicles which help people drive safely? The jury is still out on this,\n",
      "but these are the types of ethical debates that are occurring as new, innovative AI\n",
      "technology develops.\n",
      "AI impact on jobs\n",
      "While a lot of public perception of artificial intelligence centers around job losses, this\n",
      "concern should probably be reframed. With every disruptive, new technology, we see\n",
      "that the market demand for specific job roles shifts. For example, when we look at the\n",
      "automotive industry, many manufacturers, like GM, are shifting to focus on electric\n",
      "vehicle production to align with green initiatives. The energy industry isnt going away,\n",
      "but the source of energy is shifting from a fuel economy to an electric one.\n",
      "In a similar way, artificial intelligence will shift the demand for jobs to other areas. There\n",
      "will need to be individuals to help manage AI systems. There will still need to be people\n",
      "to address more complex problems within the industries that are most likely to be\n",
      "affected by job demand shifts, such as customer service. The biggest challenge with\n",
      "artificial intelligence and its effect on the job market will be helping people to transition\n",
      "to new roles that are in demand.\n",
      "Privacy\n",
      "Privacy tends to be discussed in the context of data privacy, data protection, and data\n",
      "security. These concerns have allowed policymakers to make more strides in recent\n",
      "years. For example, in 2016, GDPR legislation was created to protect the personal data\n",
      "of people in the European Union and European Economic Area, giving individuals more\n",
      "control of their data. In the United States, individual states are developing policies, such\n",
      "as the California Consumer Privacy Act (CCPA), which was introduced in 2018 and\n",
      "requires businesses to inform consumers about the collection of their data. Legislation\n",
      "such as this has forced companies to rethink how they store and use personally\n",
      "identifiable information (PII). As a result, investments in security have become an\n",
      "increasing priority for businesses as they seek to eliminate any vulnerabilities and\n",
      "opportunities for surveillance, hacking, and cyberattacks.\n",
      "Bias and discrimination\n",
      "Instances of bias and discrimination across a number of machine learning systems have\n",
      "raised many ethical questions regarding the use of artificial intelligence. How can we\n",
      "safeguard against bias and discrimination when the training data itself may be\n",
      "generated by biased human processes? While companies typically have good\n",
      "intentions for their automation efforts, Reuters (link resides outside ibm.com) highlights\n",
      "some of the unforeseen consequences of incorporating AI into hiring practices. In their\n",
      "effort to automate and simplify a process, Amazon unintentionally discriminated against\n",
      "job candidates by gender for technical roles, and the company ultimately had to scrap\n",
      "the project. Harvard Business Review (link resides outside ibm.com) has raised other\n",
      "pointed questions about the use of AI in hiring practices, such as what data you should\n",
      "be able to use when evaluating a candidate for a role.\n",
      "Bias and discrimination arent limited to the human resources function either; they can\n",
      "be found in a number of applications from facial recognition software to social media\n",
      "algorithms.\n",
      "As businesses become more aware of the risks with AI, theyve also become more\n",
      "active in this discussion around AI ethics and values. For example, IBM has sunset its\n",
      "general purpose facial recognition and analysis products. IBM CEO Arvind Krishna\n",
      "wrote: IBM firmly opposes and will not condone uses of any technology, including facial\n",
      "recognition technology offered by other vendors, for mass surveillance, racial profiling,\n",
      "violations of basic human rights and freedoms, or any purpose which is not consistent\n",
      "with our values and Principles of Trust and Transparency.\n",
      "Accountability\n",
      "Since there isnt significant legislation to regulate AI practices, there is no real\n",
      "enforcement mechanism to ensure that ethical AI is practiced. The current incentives for\n",
      "companies to be ethical are the negative repercussions of an unethical AI system on the\n",
      "bottom line. To fill the gap, ethical frameworks have emerged as part of a collaboration\n",
      "between ethicists and researchers to govern the construction and distribution of AI\n",
      "models within society. However, at the moment, these only serve to guide. Some\n",
      "research (link resides outside ibm.com) shows that the combination of distributed\n",
      "responsibility and a lack of foresight into potential consequences arent conducive to\n",
      "preventing harm to society.\n",
      "Read more about IBM's position on AI Ethics\n",
      "How to choose the right AI platform for machine learning\n",
      "Selecting a platform can be a challenging process, as the wrong system can drive up\n",
      "costs, or limit the use of other valuable tools or technologies. When reviewing multiple\n",
      "vendors to select an AI platform, there is often a tendency to think that more features =\n",
      "a better system. Maybe so, but reviewers should start by thinking through what the AI\n",
      "platform will be doing for their organization. What machine learning capabilities need to\n",
      "be delivered and what features are important to accomplish them? One missing feature\n",
      "might doom the usefulness of an entire system. Here are some features to consider.\n",
      "MLOps capabilities. Does the system have:\n",
      "a unified interface for ease of management?\n",
      "automated machine learning tools for faster model creation with low-code\n",
      "and no-code functionality?\n",
      "decision optimization to streamline the selection and deployment of\n",
      "optimization models?\n",
      "visual modeling to combine visual data science with open-source libraries\n",
      "and notebook-based interfaces on a unified data and AI studio?\n",
      "automated development for beginners to get started quickly and more\n",
      "advanced data scientists to experiment?\n",
      "synthetic data generator as an alternative or supplement to real-world data\n",
      "when real-world data is not readily available?\n",
      "Generative AI capabilities. Does the system have:\n",
      "a content generator that can generate text, images and other content\n",
      "based on the data it was trained on?\n",
      "automated classification to read and classify written input, such as\n",
      "evaluating and sorting customer complaints or reviewing customer\n",
      "feedback sentiment?\n",
      "a summary generator that can transform dense text into a high-quality\n",
      "summary, capture key points from financial reports, and generate meeting\n",
      "transcriptions?\n",
      "a data extraction capability to sort through complex details and quickly pull\n",
      "the necessary information from large documents?\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb66f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_11864\\1498168779.py:1: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  model = Gemini(models = 'gemini-pro', api_key = google_api_key)\n"
     ]
    }
   ],
   "source": [
    "model = Gemini(models = 'gemini-pro', api_key = google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f66909b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_11864\\2156247094.py:1: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  gemini_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n"
     ]
    }
   ],
   "source": [
    "gemini_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c876cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# Set global settings instead of ServiceContext\n",
    "Settings.llm = model\n",
    "Settings.embed_model = gemini_embed_model  # Also fix the typo: should be 'gemini_embed_model'\n",
    "Settings.chunk_size = 800\n",
    "Settings.chunk_overlap = 20\n",
    "\n",
    "# If you need to create an index, use it like this:\n",
    "# index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03da9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Configure settings globally\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=800, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "# No need for ServiceContext - just use Settings directly\n",
    "# Your index will automatically use these settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2a48f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 16:47:41,623 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(documents)\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:122\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mid_, doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    115\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    116\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     transformations,\n\u001b[0;32m    118\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    120\u001b[0m )\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    123\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    124\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m    125\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m    126\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    127\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m resolve_embed_model(\n\u001b[0;32m     71\u001b[0m     embed_model \u001b[38;5;129;01mor\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39membed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     78\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     79\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     80\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[0;32m     81\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m     82\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     84\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:79\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m---> 79\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_index_from_nodes(\n\u001b[0;32m     80\u001b[0m         nodes \u001b[38;5;241m+\u001b[39m objects,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:309\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content_nodes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nodes):\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome nodes are missing content, skipping them...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index_from_nodes(content_nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:278\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m     run_async_tasks(tasks)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_nodes_to_index(\n\u001b[0;32m    279\u001b[0m         index_struct,\n\u001b[0;32m    280\u001b[0m         nodes,\n\u001b[0;32m    281\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress,\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:231\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[1;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[1;32m--> 231\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_with_embedding(nodes_batch, show_progress)\n\u001b[0;32m    232\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:138\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    128\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[0;32m    129\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    130\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[0;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m embed_nodes(\n\u001b[0;32m    139\u001b[0m         nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model, show_progress\u001b[38;5;241m=\u001b[39mshow_progress\n\u001b[0;32m    140\u001b[0m     )\n\u001b[0;32m    142\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\indices\\utils.py:165\u001b[0m, in \u001b[0;36membed_nodes\u001b[1;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[1;32m--> 165\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mget_text_embedding_batch(\n\u001b[0;32m    166\u001b[0m     texts_to_embed, show_progress\u001b[38;5;241m=\u001b[39mshow_progress\n\u001b[0;32m    167\u001b[0m )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[0;32m    170\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:317\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[0;32m    320\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\core\\base\\embeddings\\base.py:473\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[1;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    469\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[0;32m    470\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[0;32m    471\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_cache:\n\u001b[1;32m--> 473\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text_embeddings(cur_batch)\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text_embeddings_cached(cur_batch)\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:472\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_retryable_get_embeddings\u001b[39m():\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[0;32m    466\u001b[0m         client,\n\u001b[0;32m    467\u001b[0m         texts,\n\u001b[0;32m    468\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _retryable_get_embeddings()\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m action(retry_state)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:465\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings\u001b[1;34m()\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_retryable_get_embeddings\u001b[39m():\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[0;32m    466\u001b[0m         client,\n\u001b[0;32m    467\u001b[0m         texts,\n\u001b[0;32m    468\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    470\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:172\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[1;32m--> 172\u001b[0m data \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlist_of_text, model\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    135\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    136\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    137\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    138\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    139\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    140\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    141\u001b[0m     ),\n\u001b[0;32m    142\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    143\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\Dhruv\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    " index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee88dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_11864\\451367967.py:18: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  Settings.llm = Gemini(model=\"gemini-2.5-flash\", api_key=google_api_key)\n",
      "C:\\Users\\Dhruv\\AppData\\Local\\Temp\\ipykernel_11864\\451367967.py:19: DeprecationWarning: Call to deprecated class GeminiEmbedding. (Should use `llama-index-embeddings-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/embeddings/google_genai/)\n",
      "  Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\", api_key=google_api_key)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "import google.generativeai as genai\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import ServiceContext, load_index_from_storage\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# Set up Settings with Gemini using CURRENT model names\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Use current Gemini 2.5 models instead of deprecated ones\n",
    "Settings.llm = Gemini(model=\"gemini-2.5-flash\", api_key=google_api_key)\n",
    "Settings.embed_model = GeminiEmbedding(model_name=\"models/text-embedding-004\", api_key=google_api_key)\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=800, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "# Load documents and create index\n",
    "documents = SimpleDirectoryReader(\"C:/Users/Dhruv/Desktop/QA_System/Data\").load_data()  # Make sure you have documents in your Data folder\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09b859ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x2c467171e00>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cccb8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a folder in the tray/ drawer with all the possible embedding types\n",
    "\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71252597",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c49501ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  query_engine.query(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7192cf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a branch of artificial intelligence and computer science that focuses on using data and algorithms to imitate human learning, progressively enhancing its accuracy. It involves training algorithms with statistical methods to make classifications or predictions and to uncover key insights in data mining projects. These insights then drive decision-making in applications and businesses.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0936b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Who is Dhruv Shah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d56532ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided information does not contain any details about Dhruv Shah.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c2c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
